# supervised learning(지도 학습)이란?

- 정답 레이블이 있는 트레이닝 셋으로 가장 흔하고, 성공적인 학습 방식**y = f(x)**x: input variable or input featurey: output or target variable→ 가능한 모든 함수의 공간은 너무 크기 때문에 바로 y를 찾는 것은 어렵다.→ hypothesis 공간에서 함수를 찾는 것이 목표
- Classification, Regression→ output y가 유한 값인지 연속적인 값인지에 따라서 결정됨
- supervised learning 2단계<1단계>→ training data set에 있는 y와 가까운 prediction을 내놓도록 모델을 fitting 하는 것<2단계>→ 모델 피팅이 끝난 후 final model에 test data에 대해서 inference를 진행해서 prediction을 놓고 성능 측정
    
    ![https://blog.kakaocdn.net/dn/cvyWwf/btrkHDetDe0/ZN8bE0fSnj4e3fRK3tQUA0/img.png](https://blog.kakaocdn.net/dn/cvyWwf/btrkHDetDe0/ZN8bE0fSnj4e3fRK3tQUA0/img.png)
    
    ![https://blog.kakaocdn.net/dn/CDZpx/btrkJ6tBN3l/Mk1KLnlXPh4yN9JuZuk1A1/img.png](https://blog.kakaocdn.net/dn/CDZpx/btrkJ6tBN3l/Mk1KLnlXPh4yN9JuZuk1A1/img.png)
    
- 학습 알고리즘supervised learning의 학습 알고리즘은 어떤 모델을 쓰느냐에 따라 달라진다.데이터 형태, 데이터 셋의 크기 등에 따라 다른 알고리즘을 사용한다.(상황에 맞게 사용)
    - Linear Regression → linear model
    - Logistic Regression → linear model
    - Support Vector Machine(SVM) → linear model
    - Naive Bayes → 확률 model
    - Gaussian Process → 확률 model
    - K-Nearest Neighbors(KNN) → training set에서 가장 가까운 neighbors를 가지고 판단하는 알고리즘
    - Decision Tree, Random Forest
    - Neural network → 딥러닝에서 사용

# Linear Model를 쓰는 supervised 알고리즘

복잡한 방식들의 기본이 되는 모델

![https://blog.kakaocdn.net/dn/MsDYC/btrkQkR6CqS/1LOCTYYBqwZ1ltSjuRHUD0/img.png](https://blog.kakaocdn.net/dn/MsDYC/btrkQkR6CqS/1LOCTYYBqwZ1ltSjuRHUD0/img.png)

d: 차원w: weightb: 1차원의 상수→ 각 차원의 숫자들과 weight를 곱하고 constant b를 더함

1. **linear regression**
    
    ![https://blog.kakaocdn.net/dn/vtT3R/btrkSOFoQHJ/XjMkA4nqZk2MLSGmi6cLT1/img.png](https://blog.kakaocdn.net/dn/vtT3R/btrkSOFoQHJ/XjMkA4nqZk2MLSGmi6cLT1/img.png)
    
2. **logistic regression**classification에서의 linear model→ decision boundary를 찾는 것!decision boundary: 두 가지의 다른 범주에 데이터가 있을 때, 갈라 주는 선을 찾는 것이 선을 찾는 알고리즘으로 logistic regression이 있다.
    
    ![https://blog.kakaocdn.net/dn/kTHgT/btrkPGU4VSH/uEU2cTFOuHVwSeOKYGbQV1/img.png](https://blog.kakaocdn.net/dn/kTHgT/btrkPGU4VSH/uEU2cTFOuHVwSeOKYGbQV1/img.png)
    
    decision boundary
    
3. **SVM(Support Vector Machine)**
    - 딥러닝 이전에 가장 인기있었던 분류학습 알고리즘
    - Maximum margin separator를 찾는 것이 목표→ (a)는 데이터를 착각할 수 있다.→ (b)처럼 margin을 두어야 좋은 decision boundary이다.→ margin에 붙어 있는 데이터들이 support vector이다.
        
        ![https://blog.kakaocdn.net/dn/dfkCwy/btrkIiVTDQO/IZACIrUiqGS6KY8K8GfKk1/img.png](https://blog.kakaocdn.net/dn/dfkCwy/btrkIiVTDQO/IZACIrUiqGS6KY8K8GfKk1/img.png)
        
    - SVM은 수학적으로 잘 정의도기 때문에 Convex Optimizaion를 통해 해결 가능Convex Optimizaion: 최적화 하고자 하는 함수가 Convex의 형태를 띈다는 것
    - linearly inseparable한 경우도 해결 가능(Kernel trick를 통해 데이터를 고차원으로 보낸 후 분류)x1, x2를 좀 더 고차원으로 보내 준 후 linear model를 사용한 SVM을 사용 해서 분류
        
        ![https://blog.kakaocdn.net/dn/zoIqL/btrkIPslzzR/zJZ6qAcC3RsKnWQVPLdlx1/img.png](https://blog.kakaocdn.net/dn/zoIqL/btrkIPslzzR/zJZ6qAcC3RsKnWQVPLdlx1/img.png)
        
        linearly inseparable
        
    - input feature가 저차원이고 학습 데이터가 적을 때 유용

# 확률 모델을 기반으로 하는 Supervised learning

1. **Naive Bayes Classification**
    - Bayes Rule(조건부 확률)이라는 확률 공식 이용input feature들이 서로 독립이라고 가정(사실은 독립이 될 가능성이 매우 적음)> p(x|y)를 어떤 확률 분포로 모델링 하냐에 따라서 Naive Bayes Classification으로 나뉨
        
        ![https://blog.kakaocdn.net/dn/5XYhx/btrkOkx4PhR/sKJzUNWRv2x9IFgsC41a51/img.png](https://blog.kakaocdn.net/dn/5XYhx/btrkOkx4PhR/sKJzUNWRv2x9IFgsC41a51/img.png)
        
    - 적은 트레이닝 데이터로도 잘 작동하고, 계산 속도가 매우 빠르다.
    - spam filtering, sentiment analysis, recommend system 등에 활용
2. **Gaussian Process**
    - 데이터 {x, f(x)}의 집합이 가우시안 프로세스에 의해서 생성되었다고 가정(f(x)들이 가우시안 분포를 이룬다고 가정)
    - 새로운 input x*에 대한 ouput x*를 조건부 확률을 통해 계산
    - 분산값을 통해 confidence를 알 수 있음
    - input feature가 고차원이거나 데이터가 많아지면 적용이 어려워짐
        
        ![https://blog.kakaocdn.net/dn/lqNlj/btrkSNGwKF9/6hu4myk1eD94w4Z2b50DCk/img.png](https://blog.kakaocdn.net/dn/lqNlj/btrkSNGwKF9/6hu4myk1eD94w4Z2b50DCk/img.png)
        

# K-Nearest Neighbos(KNN)을 사용하는 Supervised Learning

- Nonprametric approach의 대표적인 알고리즘: data↑ → parameter↑
- 새로운 input 데이터가 들어왔을 때 트레이닝 데이터 k에 가장 가까운 Neighbors를 통해서 prediction을 함→ classification이냐 regression이냐에 따라서 방법들이 달라짐
    - classification: 다수결을 통해 가장 많이 나오는 label y로 결정→ k=1일 때 overfitting이 심함→ k=5일 때 좀 더 안정적 (k는 cross validation과 같은 방법으로 찾을 수 있음)
        
        ![https://blog.kakaocdn.net/dn/8zjkJ/btrkWPcZl95/X7pwAsFJk7lH80dTZaQk4K/img.png](https://blog.kakaocdn.net/dn/8zjkJ/btrkWPcZl95/X7pwAsFJk7lH80dTZaQk4K/img.png)
        
    - Regressionclassification보다 좀 더 다양한 방식들을 취해볼 수 있음→ 왼쪽(KNN에 average 취하기)label y의 값들을 평균을 낸다. 값이 계단식으로 되기 때문에 양 바깥쪽에서 error가 심하게 난다.→ 오른쪽(KNN에 대해서 linear regression 취하기): 왼쪽과 달리 가장자리에서도 consistent하게 예측
        
        ![https://blog.kakaocdn.net/dn/dVH0gj/btrkHK570FN/49vVEkvA2kU5u9lFJilouK/img.png](https://blog.kakaocdn.net/dn/dVH0gj/btrkHK570FN/49vVEkvA2kU5u9lFJilouK/img.png)
        
- KNN의 치명적 단점 Curse of dimensionality: 고차원일 때 잘 작동하지 않는다.
- 학습 데이터가 많으면 계산 cost가 커짐

# Decision Tree, Random Forest를 사용하는 Supervised Learning

- classfication, regression에 모두 적용 가능한 간단하지만 강력한 접근법
    
    ![https://blog.kakaocdn.net/dn/bB6Km5/btrkIke40Zq/tiqTzcmI04KmzKTPhvjNB1/img.png](https://blog.kakaocdn.net/dn/bB6Km5/btrkIke40Zq/tiqTzcmI04KmzKTPhvjNB1/img.png)
    
    dicision tree
    
- 사람의 사고 방식과 가장 유사
- 계산 비용 대비 성능이 좋아 현업에서도 많이 사용
- overfitting이 되기 쉬움
- random forset는 decision tree가 많이 있다는 뜻(Ensemble이라고도 부름)
- Variance 감소 효과(overfitting 막아 줌)

# Neural Network를 사용하는 Supervised Learning

- 일반적으로 좋은 성능을 내지만 explainability는 떨어지는 black box model → hidden layer 설명 어려움
- gradient descent를 통해 학습하기 때문에 시간이 오래 걸린다 overfitting이 잘된다.
- 모델의 capacitor이 크기 때문에 overfitting이 잘된다.
- regularization을 잘쓰고 학습 데이터가 충분히 제공된다면, 좋은 성능을 낼 수 있음
- input 데이터가 고차원(이미지 포함)인 경우에도 효과적

![https://blog.kakaocdn.net/dn/cgYZoh/btrkOlw1A90/lIGszP1Cm4BQHAgIaSfnA1/img.png](https://blog.kakaocdn.net/dn/cgYZoh/btrkOlw1A90/lIGszP1Cm4BQHAgIaSfnA1/img.png)

neural network

input layer가 들어오면 각각의 노드를 이전 레이어의 모든 노드와의 연결을 통해서 weighted sum을 하고 어떤 함수 g를 거친 결과를 내도록 모델링을 해서 그 노드를 한 레이어 안에서 여러 개를 모아서 hidden layer를 만들고 또 hidden layer를 여러 층을 쌓기도 해서 최종적으로 반복적으로 계산을 통해 아웃풋 계산 하는 것

![https://blog.kakaocdn.net/dn/2AxVi/btrkHDZTxjI/gOV2SA5SBSr1eQD9zUg7N0/img.png](https://blog.kakaocdn.net/dn/2AxVi/btrkHDZTxjI/gOV2SA5SBSr1eQD9zUg7N0/img.png)

→  예측한 output 값 f(x)와 트레이닝 데이터셋의 output y를 가지고 손실함수 계산한다.

→  gradient descent를 통해서 각각의 레이어와 그 안의 노드에 weight를 들을 학습하기 때문에 neural network 방식은 explainability가 떨어지는 black box model이라고 할 수 있다. (즉 hidden layer에서 무슨 일이 일어나는지 설명하기 어려움)

# 요약

![https://blog.kakaocdn.net/dn/cKr38S/btrkSNNjkN8/tNILwAhUjcgIl26rGybNu0/img.png](https://blog.kakaocdn.net/dn/cKr38S/btrkSNNjkN8/tNILwAhUjcgIl26rGybNu0/img.png)

상황에 맞는 알고리즘을 사용하는 것이 중요
