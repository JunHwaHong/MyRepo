# Decision Tree
![화면 캡처 2022-01-09 151720](https://user-images.githubusercontent.com/44192730/148671514-1dd777de-a745-49d7-93d1-91a9a2ec0f6a.png)
- 위쪽에서 부터 아래로 노드가 엣지를 통해 이어져 있는 형태
- 한 노드가 분기 되었을 때 방향에 따라 Left Subtree 와 Right Subtree로 구분

## Decision Tree 특징

- 예측변수의 전체 공간을 단순한 여러 영역으로 계층화, 분할한다.
- 공간을 분할하는 규칙을 트리 형식으로 나타낼 수 있다.
- 단순하고 설명력이 좋아 현업에서 많이 쓰인다.
- Regression, Classification 모두 적용 가능하다.

### 장점

- 설명력이 뛰어나고, 사람의 의사결정과 닯았다.
- 이해하기 쉬운 규칙을 생성함 : if-then-else 방식
- 시각화 하기 좋다.
- 데이터의 전처리가 거의 필요하지 않고, 이상치에 덜 민감하다
- 양적, 질적 변수를 모두 다룰 수 있다.

### 단점

- 예측 정확도가 좋지는 못하다
- 강인하지 못하다 (Non-robust), input data의 작은 변화에도 최종 예측값은 크게 변화할 수 있다.
- 모든 분할은 축에 수직이다.
- 나무가 깊어질수록 과적합으로 예측력이 저하되며, 해석이 어려워진다.

## Decision Tree 분석 절차

1. 목표변수와 관련 있다고 판단되는 설명변수를 선택
2. 나무 성장 (growing) : 각 마디에서 적절한 최적의 분리 규칙(splitting rule)을 찾아 나무를 성장시킴, 정지 규칙 (stopping rule)을 만족하는 경우는 성장을 중단
3. 가지치기 (pruning) : 오류율(error rate)을 크게 할 위험이 높거나 부적절한 추론 규칙을 가지고 있는 가지를 제거
4. 타당성 평가 : 평가자료 (test data)을 이용하여 의사결정나무를 평가

## Decision Tree 알고리즘 특징
![화면 캡처 2022-01-09 151822](https://user-images.githubusercontent.com/44192730/148671548-cf94a906-8d87-4fb3-94c3-2552b47a7295.png)
![화면 캡처 2022-01-09 151833](https://user-images.githubusercontent.com/44192730/148671550-a542af57-4c45-4e14-8fc2-8006475ef698.png)

![화면 캡처 2022-01-09 151903](https://user-images.githubusercontent.com/44192730/148671563-a53ebc09-44aa-43e4-b7d3-3abdbab955c6.png)
- Input feature 대신 예측변수(predictor)라는 용어가 주로 사용됨
- 공간 분할에 대해 이해하기가 쉽다.
- 단순하고 설명력이 좋아 현업에서 많이 쓰인다.
- 연속, 불연속 변수 둘 다 사용 가능

---
## Regression Tree (회귀 나무)

- 종속변수가 연속형인 경우 사용하는 의사결정나무 모델
- 각 노드 분류에는 평균과 표준편차를 활용
- 과적합 방지 및 모델 단순화를 위해 Depth, Impurity 등 관련 설정 활용
![화면 캡처 2022-01-09 151940](https://user-images.githubusercontent.com/44192730/148671578-0273bbe0-b714-49e6-a096-623aca059e01.png)

![화면 캡처 2022-01-09 152006](https://user-images.githubusercontent.com/44192730/148671584-f83d7b7f-6c54-4efb-a16f-66ae29074089.png)
- 해당 영역에 급여 값의 평균이 leaf node의 값
- Year > Hits로 급여에 대한 결정요소 순위가 나뉜다.
- 너무 단순하긴 하지만 시각화 하기 좋다.

![화면 캡처 2022-01-09 152035](https://user-images.githubusercontent.com/44192730/148671603-827f531c-1ac0-421d-bf24-8c603385fa34.png)
- 영역을 분할하고, 영역에 평균값을 통해 예측값을 반환한다.
- 분할할 때 예측변수에 대해 P 차원에 대한 Box들로 분할한다.
- SSE을 최소화 하는 box를 찾는 것이 목표이다.

![화면 캡처 2022-01-09 152103](https://user-images.githubusercontent.com/44192730/148671611-7af3bdb7-f26e-4f98-95b5-c658fcd5a127.png)
- Top down : 꼭대기 부터 차례대로 binary split 한다.
- Greedy : 당장 한 step 후의 SSE를 최소화 하는 split을 찾는다.
→ 한번에 Split에서 SSE가 최소가 되도록

## Regression Tree의 분리규칙 탐색

#### 분산의 감소량
- 각 그룹(자식 노드) 내에서의 목표변수의 분산이 작을수록, 그룹 내 이질성이 작은 것으로 볼 수 있음.
- 자식노드로 분리했을 때 분산의 감소량이 가장 커지도록 하는 분리규칙을 탐색

#### ANOVA의 F 통계량
- F값이 클수록 그룹(자식 노드) 간에 평균차이가 있다는 것이므로, 그룹간 이질성이 큰 것으로 볼 수 있음.
- F값이 가장 커지게 되는 분리규칙을 탐색

![화면 캡처 2022-01-09 152216](https://user-images.githubusercontent.com/44192730/148671632-3ef94ac4-971e-4fd2-a61d-37507c58445a.png)
- 계속 분기를 해서 leaf 노드에 하나씩만 남게되면 SSE는 0이 되어 Training error는 좋을 수 있지만, overfitting 문제를 겪게 된다.
- 문제 해결을 위해 Stopping 조건을 이용한다.

### Decision Tree의 Overfitting
- 너무 많은 마디를 가지는 Decision Tree는 새로운 자료에 적용할 때 예측오차가 매우 커지는 Overfitting 상태가 된다.
- 이를 방지하기 위한 방법으로 정지규칙, 가지치기 방법을 사용함.

### 정지규칙 (stopping rule) : 다음의 경우에 더 이상 분리하지 않고 나무가 성장을 멈추도록 함.
- 모든 자료의 목표변수 값이 동일할때
- 마디에 속하는 자료의 개수가 일정 수준보다 적을 때
- 뿌리마디로부터의 깊이가 일정 수준 이상일 때
- 불순도의 감소량이 지정된 값보다 적을 때

### 가지치기 (pruning)
- 성장이 끝난 나무의 가지를 제거하여 적당한 크기를 가지도록 함.
- 적당한 크기를 결정하는 방법은 validation data에 대한 예측 오류가 가장 작은 나무 모형을 찾는것이 일반적.

![화면 캡처 2022-01-09 152253](https://user-images.githubusercontent.com/44192730/148671646-2313e8ab-9e53-4f6b-bcac-58a5c45975f8.png)
- Stopping 조건으로는 부족하여 Pruning(가지치기)를 사용한다.
- Cost complexity Pruning : 트리의 크기를 최소화 하며 SSE를 작게 만드려고 한다.
- α는 보통 cross validation을 통해 결정한다.

![화면 캡처 2022-01-09 152324](https://user-images.githubusercontent.com/44192730/148671659-0b03c303-1f35-417b-a4a4-3a6a50484d23.png)

---
## Classification Tree (분류 나무)
- 종속변수가 명목형인 경우 사용하는 의사결정나무 모델
- 각 노드 분류 알고리즘은 이진 분류 시 지니지수(Gini index) 기반의 CART 사용
- 과적합 방지 및 모델 단순화를 위해 Depth, Impurity 등 관련 설정 활용

![화면 캡처 2022-01-09 152431](https://user-images.githubusercontent.com/44192730/148671691-a409bf83-db9d-49fc-a09c-a5e4b149de6e.png)
- Leaf node에서 가장 많이 등장하는 class가 예측 class가 된다.
- SSE가 아닌 Classification error rate, Gini index, Entropy 등을 척도로 사용한다.
- Classification error rate의 경우 최대 분류만 신경쓴다.
- Gini index, Entropy의 경우 골고루 잘 분포 되도록 하는 척도
- Pruning에서는 최종 예측의 정확도를 위해 Classification error rate를 주로 사용.

## Entropy (엔트로피)
![화면 캡처 2022-01-09 152510](https://user-images.githubusercontent.com/44192730/148671705-da85c4da-e120-4eff-817f-6c087ae4615d.png)

![화면 캡처 2022-01-09 152545](https://user-images.githubusercontent.com/44192730/148671717-3abbc4db-ce3a-4930-84cc-04e85621ce0f.png)
![화면 캡처 2022-01-09 152558](https://user-images.githubusercontent.com/44192730/148671719-a1d36510-7762-4252-bec2-1687dc6410d7.png)

![화면 캡처 2022-01-09 152646](https://user-images.githubusercontent.com/44192730/148671738-f1636595-ad61-4c5b-9183-4b6583140a67.png)

![화면 캡처 2022-01-09 152711](https://user-images.githubusercontent.com/44192730/148671749-0ad2f65b-198a-4838-a022-8a93285a7f53.png)
- 최종 트리를 만들고 Pruning을 위해 Cross validation을 한다.

---
## Linear Model vs Decision Tree
![화면 캡처 2022-01-09 152723](https://user-images.githubusercontent.com/44192730/148671756-5f25504e-4464-4084-b5bd-b094aa0158bb.png)
- Decision boundary형태를 고려하여 Linear Model, Decision Tree 를 사용해야 한다.

![화면 캡처 2022-01-09 152828](https://user-images.githubusercontent.com/44192730/148671765-dc0ac1f2-64a5-4142-a6c7-ecb9cc36413b.png)
## 주요 함수, 메서드
- Sklearn - DecisionTreeClassifier(), DecisionTreeRegressor()
- Hyper Parameter
- max_depth : 최대 가지치기 수
- max_leaf_node : 리프 노드의 최대 개수
- min_sample_leaf : 리프 노드가 되기 위한 최소 샘플 수

