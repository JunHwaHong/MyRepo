# DBSCAN
### (Density-based spatial clustering of applications with noise)

[데이터 사이언스 스쿨](https://datascienceschool.net/03%20machine%20learning/16.03%20%EB%94%94%EB%B9%84%EC%8A%A4%EC%BA%94%20%EA%B5%B0%EC%A7%91%ED%99%94.html)

- 클러스터링 알고리즘중 밀도 방식의 클러스터링을 사용
- K Means나 Hierarchical 클러스터링의 경우 군집간의 거리를 이용하여 클러스터링을 하는 방법인데, 밀도 기반의 클러스터링은 점이 세밀하게 몰려 있어서 밀도가 높은 부분을 클러스터링 하는 방식
- 어느점을 기준으로 반경 x내에 점이 n개 이상 있으면 하나의 군집으로 인식

---
## 용어 정의

- Density at a data point x: 반지름이 ε인 원 안에 data point 개수
- Dense region: 최소한의 data point 개수(minPts)를 포함하는 반지름 ε의 원 (최소한의 data point 는 정할 수 있음)
- Neighborhood: 두 data point 간의 거리가 반지름 ε 이하인 경우
- Core point: 반지름 ε 안에 minPts 이상의 데이터를 포함하는 data point
- Border point: 반지름 ε 안에 minPts 이상을 포함하지만, core point와 neighborhood 인 data point
- Noise point: core point, border point 둘 다 아닌 data point
- Density edge: 두 data point가 neighborhood인 경우 둘 사이에 density edge를 둠
- Density-connected: 두 data point가 일련의 edge들을 통해 연결되는 경우

---
## 수행절차
1. Data point들을 core, border, noise 중 하나로 labeling한다.
2. Noise points 들을 없앤다.
3. 군집에 assign 되지 않은 모든 core point x 에 대해 다음을 반복: Point x 및 Point x와 density-connected 된 모든 point 들에 새로운 군집을 assign
4. Border point 는 가장 가까운 core point의 군집으로 assign 한다.

![화면 캡처 2022-01-09 161559](https://user-images.githubusercontent.com/44192730/148673014-f45cefa4-088f-4b1f-84cf-f66ace1d32fe.png)

#### 참고자료
[클러스터링 #3 - DBSCAN (밀도 기반 클러스터링)](https://bcho.tistory.com/1205)

![화면 캡처 2022-01-09 161643](https://user-images.githubusercontent.com/44192730/148673030-b9e6162d-980a-4cb9-9484-dca9da91b516.png)
## DBSCAN 장단점

1. 장점
- K Means와 같이 클러스터의 수를 정하지 않아도 됨
- Cluster의 밀도에 따라서 클러스터를 서로 연결하기 때문에 기하학적인 모양을 갖는 군집도 잘 찾을 수 있음
- Noise point를 통하여, outlier 검출이 가능하다. (Noise에 취약하지 않음)
2. 단점
-  Density가 다른 경우 잘 안될 수 있음

## Practice

[Exploring Clustering Algorithms: Explanation and Use Cases - neptune.ai](https://neptune.ai/blog/clustering-algorithms)

```python
from numpy import where
from numpy import unique
from sklearn.datasets import make_classification
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plot

# Initialize  data
train_data, _ = make_classification(n_samples=1000,
                                       n_features=2,
                                       n_informative=2,
                                       n_redundant=0,
                                       n_clusters_per_class=1,
                                       random_state=4)

# Define model
dbscan_model = DBSCAN(eps=0.25, min_samples=9)

# Train the model
dbscan_model.fit(train_data)

# Assign each data point to a cluster
dbscan_res = dbscan_model.fit_predict(train_data)

# obtain all the unique clusters
dbscan_clstrs = unique(dbscan_res)

# Plot the DBSCAN clusters
for dbscan_clstr in dbscan_clstrs:
    # Obtain data point that belong in this cluster
    index = where(dbscan_res == dbscan_clstr)
    # plot
    plot.scatter(train_data[index, 0], train_data[index, 1])
    
# show plot
plot.show()
```

![https://i0.wp.com/neptune.ai/wp-content/uploads/DBSCAN-clustering-results.png?resize=370%2C248&ssl=1](https://i0.wp.com/neptune.ai/wp-content/uploads/DBSCAN-clustering-results.png?resize=370%2C248&ssl=1)

# Clustering Practice

[2.3. Clustering](https://scikit-learn.org/stable/modules/clustering.html)

## 가장 유명한 clustering algorithm 종류

1. Affinity Propagation : 모든 데이터가 특정한 기준에 따라 자신을 대표할 대표 데이터를 선택. 클러스터의 중심 = 스스로가 자기 자신을 대표하는 경우
2. Agglomerative Clustering : (= 계층적 군집화 hierarchical clustering) 여러개의 클러스터 중에서 가장 유사도가 높은 / 거리가 가까운 군집 두 개를 선택하여 하나로 합치면서 군집 개수를 줄여 가는 방법
3. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) : 한 번만 데이터를 검사하여 클러스터를 만들며, 모든 데이터를 스캔하지 않고도 클러스터링을 결정을 내린다는 점에서 local한 방법
4. DBSCAN
5. K-Means
6. Mini-Batch K-Means
7. Mean Shift : 정해진 크기의 window 반경 안에서 데이터의 분포가 높은 곳으로 단계적으로 중심점을 이동하며 군집을 완성하는 방식
8. OPTICS (Ordering Points To Identify the Clustering Structure) : DBSCAN과 유사하지만, 다양한 밀도를 가진 데이터에서 클러스터를 잘 찾아내지 못하는 문제를 해결하기 위해 고안. 데이터를 정렬하여 가까운 point들이 이웃이 될 수 있도록 하며, 각 point가 더 밀집된 cluster에 포함될 수 있도록 함
9. Spectral Clustering : 그래프 기반 클러스터링. 데이터들 간의 상대적인 관계나 연결을 가장 중요한 정보로 이용
10. Mixture of Gaussians : 데이터의 군집을 가우시안 모델로 표현하는 기법. 가우시안 모델의 평균과 분산으로부터 군집의 특성을 알 수 있음. 데이터의 분포로부터 가우시안 분포를 선형 결합한 가정을 하고 클러스터링 함.

# Summary

- Unsupervised Learning 은 데이터의 구조를 이해하는데 매우 중요
- supervised learning을 위한 전처리, 데이터 시각화 등에도 유용
- 실제 많은 데이터가 unlabeled 되어있음에 따라 활용 가능성 높은 학습 방법
- 비지도학습으로 수행가능한 Task:
- Dimensionality Reduction : 고차원으로 표현된 데이터를 저차원으로 설명 (방법 : PCA, MDS, t-SNE)
- Clustering : 데이터 안에서 군집을 찾는 것 (방법: K-Means, Agglomerative, DBSCAN)
- Unsupervised learning은 정답 데이터 / 목적이 정해져 있지 않기 때문에 supervised learning보다 훨씬 어려움 -> 활발한 연구분야로서, 딥러닝을 이용하는 다양한 방법들이 계속해서 나오고 있음
