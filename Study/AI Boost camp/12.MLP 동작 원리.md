# MLP 동작 원리

## Forward Pass
#### 1. Forward Pass란?
    -입력이 주어진 상태에서 MLP(Multi-Layer Perception) 모델이 실제 true label 추론하는 과정
    - Parameter와 activation function을 이용해 각 입력에 대한 출력을 함수의 형태로 얻음
    
![화면 캡처 2022-01-09 210750](https://user-images.githubusercontent.com/44192730/148681575-c586c3f1-5c1b-4ff0-96e7-c66b013fc912.png)

    - 각 층에서 입력이 weights와 곱해지는 선형적인 형태를 가지기 때문에 행렬 곱의 형태로 간단하게 표현 가능 
    - 행 벡터를 기준으로 생각하기에 weight의 묶음은 transpose형식으로 나타냄
     
#### 2. Forward Pass에서 MLP가 되려면?
    - Single-Layer Perception에서 hidden Layer로의 출력값이 증가하고, 모든 노드에 Weight 생성
    - hidden Layer 노트 하나가 Single-Layer Perception과 같은 행렬 곱으로 나타남
    
![화면 캡처 2022-01-09 211006](https://user-images.githubusercontent.com/44192730/148681598-d1be649e-8ddc-4e09-b04b-b0804f9b0eca.png)

3. Batch Training
    - machine learning의 학습과 추론은 효율성의 이유로 여러 개의 데이터로 묶어서 진행함
    - 이러한 데이터 묶음 = 'batch'
    - machine learning과 deep learning framework는 batch 단위의 operation이 가능

4. Matrix Multiplication
    - 2차원 행렬들의 곱
    - machine learning과 deep learning에서 parameter와 data 사이의 가장 기본적인 연산
    → deep learning의 성공을 가능케 함
    
![화면 캡처 2022-01-09 211109](https://user-images.githubusercontent.com/44192730/148681637-68e3ba7b-7334-41b6-a3ca-0892dd0252f0.png)

5. Matrix Multiplication을 이용한 Batch Training
    - 하나의 데이터를 벡터(x)로 표현했을 때, 데이터의 묶음(batch)를 행렬로 표현 → parameters와의 곱을 행렬과 행렬의 곱으로 표현
        
![화면 캡처 2022-01-09 211228](https://user-images.githubusercontent.com/44192730/148681679-5152cc6a-7dce-4215-8200-1544858fe446.png)

6. Mini-Batch Training
    - 효율적인 학습을 진행하는 동시에 학습과정에 randomness를 추가해주기 위해 데이터 셋 전체를 하나의 batch로 보는 것이 아니라 그 일부를 mini-batch로 취급 → 전체 데이터 셋으로 여러 번의 학습 과정을 진행
     - data 셋을 전부 넣으면?           
            1) 실제 학습 data에 너무 맞는 업데이트 → validation과 test데이터에 대해 generalization이 잘 되지 않아 over-fitting 가능성이 높아짐
            2) 메모리 문제 
            → 이 두 가지 문제로 인해 batch를 또 나누는데 이러한 batch를 mini-batch라고 부름. (mini-batch 또한 random sampling을 통해 모델에 혼동을 주고 이 과정에서 모델의 추론 능력과 성능 향상이 가능)
            
    - 1 epoch = 전체 데이터 셋으로 학습을 진행한 것 → 전체 학습 과정은 여러 epoch를 가짐
        - 각 epoch →  여러 개의 mini-batch를 통한 학습을 포함
        - 1 epoch = Data set Size / mini-batch size 번의 업데이트로 진행
        
7. 각 layer에서의 연산 형태
    - 데이터로 batch를 구성하면 2차원 이상의 행렬이 되기 때문에 각 layer에서의 연산 = 행렬과 행렬의 곱 형태를 취함

![화면 캡처 2022-01-09 211358](https://user-images.githubusercontent.com/44192730/148681712-8c1f8cdc-ab82-4476-a774-4b53c0519545.png)


