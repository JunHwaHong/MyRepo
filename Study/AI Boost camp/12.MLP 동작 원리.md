# MLP 동작 원리

## Forward Pass
#### 1. Forward Pass란?
    -입력이 주어진 상태에서 MLP(Multi-Layer Perception) 모델이 실제 true label 추론하는 과정
    - Parameter와 activation function을 이용해 각 입력에 대한 출력을 함수의 형태로 얻음
    
![화면 캡처 2022-01-09 210750](https://user-images.githubusercontent.com/44192730/148681575-c586c3f1-5c1b-4ff0-96e7-c66b013fc912.png)

    - 각 층에서 입력이 weights와 곱해지는 선형적인 형태를 가지기 때문에 행렬 곱의 형태로 간단하게 표현 가능 
    - 행 벡터를 기준으로 생각하기에 weight의 묶음은 transpose형식으로 나타냄
     
#### 2. Forward Pass에서 MLP가 되려면?
    - Single-Layer Perception에서 hidden Layer로의 출력값이 증가하고, 모든 노드에 Weight 생성
    - hidden Layer 노트 하나가 Single-Layer Perception과 같은 행렬 곱으로 나타남
    
![화면 캡처 2022-01-09 211006](https://user-images.githubusercontent.com/44192730/148681598-d1be649e-8ddc-4e09-b04b-b0804f9b0eca.png)

3. Batch Training
    - machine learning의 학습과 추론은 효율성의 이유로 여러 개의 데이터로 묶어서 진행함
    - 이러한 데이터 묶음 = 'batch'
    - machine learning과 deep learning framework는 batch 단위의 operation이 가능

4. Matrix Multiplication
    - 2차원 행렬들의 곱
    - machine learning과 deep learning에서 parameter와 data 사이의 가장 기본적인 연산
    → deep learning의 성공을 가능케 함
    
![화면 캡처 2022-01-09 211109](https://user-images.githubusercontent.com/44192730/148681637-68e3ba7b-7334-41b6-a3ca-0892dd0252f0.png)

5. Matrix Multiplication을 이용한 Batch Training
    - 하나의 데이터를 벡터(x)로 표현했을 때, 데이터의 묶음(batch)를 행렬로 표현 → parameters와의 곱을 행렬과 행렬의 곱으로 표현
        
![화면 캡처 2022-01-09 211228](https://user-images.githubusercontent.com/44192730/148681679-5152cc6a-7dce-4215-8200-1544858fe446.png)

6. Mini-Batch Training
    - 효율적인 학습을 진행하는 동시에 학습과정에 randomness를 추가해주기 위해 데이터 셋 전체를 하나의 batch로 보는 것이 아니라 그 일부를 mini-batch로 취급 → 전체 데이터 셋으로 여러 번의 학습 과정을 진행
     - data 셋을 전부 넣으면?           
         1) 실제 학습 data에 너무 맞는 업데이트 → validation과 test데이터에 대해 generalization이 잘 되지 않아 over-fitting 가능성이 높아짐
            
         2) 메모리 문제 
         → 이 두 가지 문제로 인해 batch를 또 나누는데 이러한 batch를 mini-batch라고 부름. (mini-batch 또한 random sampling을 통해 모델에 혼동을 주고 이 과정에서 모델의 추론 능력과 성능 향상이 가능)
            
    - 1 epoch = 전체 데이터 셋으로 학습을 진행한 것 → 전체 학습 과정은 여러 epoch를 가짐
        - 각 epoch →  여러 개의 mini-batch를 통한 학습을 포함
        - 1 epoch = Data set Size / mini-batch size 번의 업데이트로 진행
        
7. 각 layer에서의 연산 형태
    - 데이터로 batch를 구성하면 2차원 이상의 행렬이 되기 때문에 각 layer에서의 연산 = 행렬과 행렬의 곱 형태를 취함

![화면 캡처 2022-01-09 211358](https://user-images.githubusercontent.com/44192730/148681712-8c1f8cdc-ab82-4476-a774-4b53c0519545.png)

---
## Activation Function이 필요한 이유?
![화면 캡처 2022-01-09 211920](https://user-images.githubusercontent.com/44192730/148681886-f4371296-0908-4a1f-b8ed-6d3301470b56.png)

![화면 캡처 2022-01-09 211957](https://user-images.githubusercontent.com/44192730/148681896-77d1bb86-3db4-4dd9-b476-a03088af7de7.png)

- O(outbput) 은 각 hiden-layer에서의 가중치들의 곱을 한 X(input)이 되어 선형적인 성격을 가지게 됨
- Sing-layer perception을 만들어 표현이 가능하기 때문에 무수한 layer 도출 과정이 필요가 없음
       →  MLP 로 여러 hidden-layer 처리과정이 필요가 없음
       
![화면 캡처 2022-01-09 212037](https://user-images.githubusercontent.com/44192730/148681919-9570daf4-e323-4080-bb75-8226266b9881.png)

- Layer 처리 때 비선형 함수 ‘a(X)’ 비선형적인 activation function 처리 과정을 추가하여 선형적인 성격을 없애 주고 표현력을 늘릴 수 있음

![화면 캡처 2022-01-09 212109](https://user-images.githubusercontent.com/44192730/148681940-e8206c81-3458-4b38-995e-b1d255fb7e8b.png)

- 딥러닝 모델 학습에서 비선형 성질의 activation function 사용할때 유용한 조건
- MLP를 forward pass를 하는데 많이 사용될 뿐 만아니라 back propagation 처리 과정에서도 activation function이 존재하며 해당 activation function 처리 과정에서 미분계산을 처리해야하는 부분이 필요함
- Activation function이 작동하는 값(비선형적 구간)의 범위 자체가 실제 데이터 분포와 맞아야함 →  비선형적 부분에 포함되지 않는다면 선형적 데이터 성격을 띄기 때문에 비선형적 activation function 처리 과정이 무의미해짐

* 참고 back propargation

![화면 캡처 2022-01-09 212239](https://user-images.githubusercontent.com/44192730/148681993-8f55a070-71e0-43a9-b53e-4adebd2b7923.png)

[4. Activation Function은 왜 만들었나요?](https://89douner.tistory.com/22)

## 딥러닝의 비선형 Activation function

![화면 캡처 2022-01-09 212337](https://user-images.githubusercontent.com/44192730/148682036-8af70268-e386-4783-9015-4231fc3050a4.png)'

- 특징 : 0<y<1 값을 가지는 함수
- 장점 :
    Sigmoid function이 많이 쓰였던 이유 : 미분계수의 계산이 간단함
    미분계수가 다시 해당 미분계산식으로 표현이 가능하기 때문에 back propargation을 할 때 유용한 함수
    미분계수 증명: [https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x/1225116#1225116](https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x/1225116#1225116)
    
- 단점 : 미분계수가 0으로 수렴하는 구간이 넓기 때문에 Vanishing Gradient 문제가 발생

![화면 캡처 2022-01-09 212544](https://user-images.githubusercontent.com/44192730/148682106-e543185e-aacf-4970-b3e3-3b685ae308a7.png)

![화면 캡처 2022-01-09 212618](https://user-images.githubusercontent.com/44192730/148682125-e8cae1c4-1a8d-4e83-8804-afac34e6702a.png)

- 음수 영역은 0으로 수렴
- 양수의 영역에서 기울기가 1이기 때문에 sigmoid 함수의 단점이였던 Vanishing Gradient 문제를 보완 가능함
- Max operation 처리 부분과 미분 계산 시 굉장히 빠름

![화면 캡처 2022-01-09 212653](https://user-images.githubusercontent.com/44192730/148682137-a3cbfca8-3949-4304-aaaa-a81959b4a18c.png)

- ReLU function의 음의 영역을 살리기 위해 양수의 알파값을 곱하여 max operation을 계산 ( 위의 그래프에서 알파값은 1)
- 알파 값을 hyperparamater로 선택을 해줘야함
- 알파 값 5개를 테스트를 하게 되면 실험을 5배를 해야하기 때문에 소요되는 시간과 비용이 커지지만 기존 ReLU function과 성능차이가 크지가 않음
- elu, Prelu 등 ReLU 계열 함수들이 존재
- max out : ReLU 계열 함수를 일반화하는 방법론의 함수

[딥러닝에서 사용하는 활성화함수](https://reniew.github.io/12/)

![화면 캡처 2022-01-09 212733](https://user-images.githubusercontent.com/44192730/148682163-82739560-9e3e-4f12-84a3-3bc5dab009d9.png)

- 보통 모델 중간에 들어가기 보단 모델 맨 뒤에 사용하는 비선형적 activation function
- Output 백터값의 elemenet 끼리 연관이 없기때문에 O_1과 O_2 간 만족해야 되는 조건이 없음
- 앞쪽 노드(혹은 layer) 값에서 각각 weight를 곱한다음 linear combination을 해준 값
- Output 값들을 확률분포로 변환하면 output 백터의 합은 1이 되는 조건으로 함수를 설정
- Output 값에다가 전부 다 exponential을 해가지고 음수값이 큰 문제가 되지않도록 한 다음 그것을 다 더해서 normalization 하는 방식
- 모델이 어떤 class에 속할 확률적인 근거로써 해석 가능하게 함 → Classification task에서 마지막 layer에서 무조건적으로 사용하게 되는 function

[Sigmoid, Logit and Softmax](https://chacha95.github.io/2019-04-04-logit/)

---
## Loss Function

![화면 캡처 2022-01-09 212827](https://user-images.githubusercontent.com/44192730/148682187-d4f3f2c7-e9d3-4d16-b554-cf3efa9edab8.png)

- Loss function (a.k.a. cost or objective)
    - 모델의 output($\hat y$) 와 실제값 $y$ 가 얼마나 차이나는지 나타내는 척도 = loss function
    - Backward Pass 에서 각 모델의 parameter(weights)들을 업데이트하는 기준
- 강화학습에서는 loss가 아니라 reward라고 부름
    - reward는 클수록 좋음
- 해결하려는 문제에 맞는 loss function을 찾는 것이 중요
    - 미분의 형태
- pytorch loss function

[torch.nn - PyTorch 1.10.0 documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)

![화면 캡처 2022-01-09 212908](https://user-images.githubusercontent.com/44192730/148682201-3f774158-9681-4039-80d1-647554d53274.png)

- 이차식 형태의 loss function
    - 에러값이 클수록 loss값이 quadratic이 커진다.
    - error 가 큰 data point or dimension에서 먼저 에러를 줄이는 방향으로 학습하게 된다.
    - 전체적으로 error 값들이 비슷한 값들을 갖도록 진행된다.
- L1-norm
    - $L = \sum_{i=1}^{n}|y_i-\hat y_i|$
    - loss가 작은 곳부터 최적화를 시키는 경향
    - 0에서 미분잉 불가능함
    - L2 Loss 보다 outlier의 영향을 덜 받음
    - MSE 보다는 덜 사용
    - 참고자료 L1 vs L1 >
    
    [L1, L2 Norm, Loss, Regularization?](https://junklee.tistory.com/29)
    
![화면 캡처 2022-01-09 212945](https://user-images.githubusercontent.com/44192730/148682220-1c66ad19-364c-4874-acfd-f93f7359577d.png)

- classfication task 에서 사용
    - softmax function이 적용되면  ouput가 확률 분포처럼되고 cross entropy 에 바로 넣어줄 수 있음\
    - $y_i$ : true label / $\hat y$: ouput
