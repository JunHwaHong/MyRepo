# Backward Pass Basic Operations

- Backward pass : MLP의 forward pass에 사용되는 각각의 연산에 대해 chain rule이 어떻게 적용되는지 분석하여 전체 MLP에 순차적으로 반대 방향으로 적용
- Chain rule에 따라 머신러닝 모델은 임의의 hyperparameter 값을 사용하여 예측 (= 정방향 전파)
    
    → 임의의 초기 값을 조정하면서 실제 값과 비교하여 오류를 최소화하려고 시도 (= backward propagation, 역전파)
    
- 국소적 계산을 전파하여 최종 결과를 얻음

## 신경망 학습의 전체 그림 (4 단계)

- 학습: 신경망의 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정
1. 미니배치 : 훈련 데이터 중 일부를 무작위로 가져옴
2. 기울기 산출 : 각 가중치 매개변수의 기울기 (손실 함수의 값을 작게 하는 방향)를 구함
→ 오차역전파법이 등장하는 단계
3. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신
4. 반복 : 1 ~ 3단계를 반복

## 기울기 확인 Gradient Check

- 기울기를 구하는 방법
    1. 수치 미분
    2. 오차역전파법
- 기울기 확인 : 오차역전파법을 정확히 구현했는지 수치 미분으로 확인

---
## MLP의 연산
1. Matrix Multiplication (Addition, Multiplication, Common Variable)
2. Nonlinear Activation Function (Sigmoid, ReLU)

## Matrix Multiplication
### Addition Operation

- MLP의 forward pass는 matrix multiplication과 nonlinear activation function으로 구성
- 각 node로 parameter와 이전 층의 값이 곱해져 더해질 때 등장
- 결론 : 덧셈 노드의 역전파는 상류의 값을 그대로 흘러보냄

![화면 캡처 2022-01-16 151753](https://user-images.githubusercontent.com/44192730/149649616-911be814-9a9e-4c6d-ac63-57e1be648235.png)

### Multiplication Operation

- MLP의 forward pass는 matrix multiplication과 nonlinear activation function으로 구성
- 각 layer에서 parameter와 이전 층의 값이 곱해질 때 등장
- 결론 : 곱셈노드 역전파는 상류의 값에 순전파 때의 입력 신호들을 ‘서로 바꾼 값'을 곱해서 하류로 보냄

![화면 캡처 2022-01-16 151838](https://user-images.githubusercontent.com/44192730/149649633-a821716d-272b-4f6d-acee-2b8965325338.png)

### Common Variable

- MLP의 forward pass는 matrix multiplication과 nonlinear activation function으로 구성
- 이전 층의 값이 각 parameter와 곱해질 때 등장

![화면 캡처 2022-01-16 151912](https://user-images.githubusercontent.com/44192730/149649647-b2f2f7ee-0624-4096-9a94-12ae75a6d280.png)

## Nonlinear Activation Function

- MLP의 forward pass는 matrix multiplication과 nonlinear activation function으로 구성
- 각 node에서 activation function이 사용될 때 등장

![화면 캡처 2022-01-16 151959](https://user-images.githubusercontent.com/44192730/149649661-bb3afa41-3dc4-4a8a-832e-850e663312e5.png)

- Activation function의 backward pass를 위해서는 해당 node에서 각 함수의 미분값이 필요
    - Activation function은 element-wise로 계산이 이루어지기 때문에 벡터나 행렬을 고려하지 않고 간단하게 처리

### Sigmoid

![화면 캡처 2022-01-16 152042](https://user-images.githubusercontent.com/44192730/149649680-2c3e8a54-198b-42c5-b04b-46e4114f0ca7.png)

- y = 1/x 를 미분하면 -y²

→ 즉, 역전파 때 상류에서 들어온 값에 -y²를 곱해서 하류로 전달 (순전파의 출력의 제곱에 마이너스를 붙인 값)

![화면 캡처 2022-01-16 152345](https://user-images.githubusercontent.com/44192730/149649748-44cceb9e-ef08-4d58-8c1b-3c9207cf2a2b.png)

### ReLu

![화면 캡처 2022-01-16 152420](https://user-images.githubusercontent.com/44192730/149649761-71c03433-4c7e-4399-a68a-cb67be0f4d68.png)

- 순전파 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘림
- 순전파 입력 x가 0이하이면 역전파 때 하류로 신호를 보내지 않음

![화면 캡처 2022-01-16 152447](https://user-images.githubusercontent.com/44192730/149649772-c2a82b5e-5ca3-4377-b7a6-b3927fb81980.png)




