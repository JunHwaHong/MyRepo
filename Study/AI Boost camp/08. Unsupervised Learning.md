# Unsupervised Learning (비지도 학습)
![화면 캡처 2022-01-09 153721](https://user-images.githubusercontent.com/44192730/148671942-e4315b2a-f95b-472f-a12e-68a0edb417ab.png)

![화면 캡처 2022-01-09 153744](https://user-images.githubusercontent.com/44192730/148671949-e485cc69-84b8-4592-b2a8-18dacf7ffb68.png)

---
## Dimentionality Reduction(차원 축소)이 필요한 이유
- **차원의 저주**
**
현실 세계에서 머신러닝을 적용시키려고 할 때 우리는 고차원의 데이터를 자주 마주하게 됩니다. 영화 추천 시스템을 예로 들 경우 데이터의 형태는 (# of users) x (# of movies) 행렬로 사용자와 영화의 수에 따라서 데이터의 차원이 기하급수적으로 증가하게 됩니다.

차원이 증가하면서 차원의 저주(Curse of dimensionality) 문제가 발생되는데 이는 차원이 증가하면서 변수는 증가하지만 관측 데이터는 그대로이기 때문에 차원내 학습할 데이터의 수가 작아지면서 발생하는 문제점 입니다.
![화면 캡처 2022-01-09 153846](https://user-images.githubusercontent.com/44192730/148671967-db502fa3-1517-4a25-b77c-fba324d1448e.png)

- 노이즈 제거 (모델의 설명력)
높은 차원의 데이터, 즉 feature를 여러개 사용하여 모델을 학습시켰을 때 학습 데이터에 overfitting되는 문제가 발생할 수 있습니다. 즉 특정 데이터들은 저차원에서 학습 정확도는 조금 낮지만 새로운 데이터에 대한 정확도가 더 높을 수 있습니다. 이는 많은 feature들 중에 모델의 성능에 중요도가 높은 feature들이 많지 않을 때 발생할 수 있습니다. 
이러한 문제가 있을 때 우리는 차원 축소 방법을 사용해서 고차원의 데이터에서 모델에 필요한 중요 feature들만 뽑아내는 것으로 모델의 성능을 향상 시킬 수 있습니다.
예를 들어 MNIST 손 글씨 데이터를 2차원으로 축소하면 아래와 같이 시각적으로 확인이 가능할 만큼 클러스터링이 잘 되는 것을 볼 수 있습니다.

![화면 캡처 2022-01-09 153917](https://user-images.githubusercontent.com/44192730/148671984-2fc13d52-57e9-462c-8228-8ca062661d89.png)
- 그 밖에도 차원을 축소함으로써 **시각화**가 가능해지는 장점도 있고 쓸모없는 feature를 제거하면서 **메모리를 절약**하는 효과도 있습니다.

---
### 목적에 따른 다양한 방법 존재
- PCA : 데이터의 variance를 보존하면서 차원 축소
- MDS : 데이터간의 거리 정보를 보존하면서 차원 축소
- t-SNE : 로컬 거리 정보를 보존하면서 차원 축소 및 시각화 목적
- 딥러닝 기반의 차원 축소 : Auto-encoder, Word2Vec

---
### 차원 축소 방법
#### PCA : 데이터의 variance를 보존하면서 차원 축소
PCA(Principal Component Analysis)란, 여러 데이터들이 모여 하나의 분포를 이룰 때 이 분포의 주 성분(Principal Component)를 분석해 주는 방법입니다. 즉 여러 feature들을 모아서 이 feature들의 분포를 가장 잘 설명할 수 있는 새로운 feature를 찾아내는 방법이라고 할 수 있습니다.
여기서 말하는 주 성분이란, 데이터들의 분산이 가장 큰 방향 벡터를 의미 합니다. 3차원 데이터를 예로 들면 아래 그림과 같이 서로 수직이면서 데이터들의 분산(흩어진 정도)가 가장 큰 $e_{1}, e_{2}, e_{3}$의 주 성분을 찾아낸 것을 확인할 수 있습니다.

![화면 캡처 2022-01-09 154048](https://user-images.githubusercontent.com/44192730/148672030-bd0eb927-bec0-4200-8032-f39ce08e519d.png)
- PCA는 입력 데이터들의 공분산 행렬(covariance matrix)에 대한 고유값 분해(eigen decomposition)이라고 볼 수 있습니다. 여기서 공분산 행렬이란 입력 데이터의 좌표 성분들 사이의 공분산 값을 원소로하는 행렬입니다. 그렇다면 공분산이란 뭘 의미 할까요? 공분산은 x, y의 분산(흩어진 정도)의 상관관계를 나타냅니다. 
- 데이터 행렬 X에 대한 공분산 행렬은 다음과 같습니다.
$\sum = \frac{1}{n}X^{T}X$
- 고유 벡터는 그 행렬이 벡터에 작용하는 주축의 방향을 나타내고 따라서 공분산 행렬의 고유 벡터는 데이터가 어떤 방향으로 분산되어 있는지를 나타내준다고 할 수 있습니다.

![화면 캡처 2022-01-09 154156](https://user-images.githubusercontent.com/44192730/148672054-7a34a8ca-253b-48c3-8287-4eb881f9057d.png)

![화면 캡처 2022-01-09 154737](https://user-images.githubusercontent.com/44192730/148672186-ec7d0c97-dec1-4cff-931c-b609351b2dc9.png)


#### PCA의 한계점
- Variance에 초점이 맞춰져 있다.
- Classification에 도움이 되지 않을 수 도 있다.
- 데이터의 분포가 non-Gaussian일 때 잘 안된다.

---

#### MDS : 데이터간의 거리 정보를 보존하면서 차원 축소

다차원 척도법 (Multidimensional Scaling, MDS)의 목적은 d 차원의 공간 상에 있는 객체의 거리 정보를 최대한 보존하는 저차원의 좌표계를 찾는 것 입니다. 즉, 거리 정보를 보존하면서 차원을 축소하는 것 입니다.
입력 데이터로는 n개의 객체 간의 근접도 행렬 (n x n)이고 출력은 각 객체의 좌표값으로 수행 절차를 2 단계로 설명할 수 있습니다.

1) 거리 행렬 $D$ 만들기
- 거리를 계산하기 위해서 유클리드 거리 또는 맨하탄 거리를 사용합니다. 유클리드 거리를 사용했을 때 행렬 각 요소의 값은 아래의 식으로 구할 수 있습니다.
 $d^{2}_{rs} = (x_{r} - x_{s})^{T}(x_{r} - x_{s})$

2) 좌표계 찾기
- $D$로부터 $b_{rs} = X_{r}^{T}X_{s}$인 n x n의 matrix B를 계산하고, $B=XX^{T}$인 n x d matrix $X$를 계산합니다.

---
#### t-SNE : 로컬 거리 정보를 보존하면서 차원 축소
- Local neighborhood를 잘 보존하면서 차원 축소 하는 것에 초첨이 맞춰진 방법
- 차원의 복잡한 데이터를 2차원에 차원 축소하는 방법.
- 낮은 차원 공간의 시각화에 주로 사용하며 차원 축소할 때는 비슷한 구조끼리 데이터를 정리한 상태이므로 데이터 구조를 이해하는 데 도움을 준다.

t-SNE는 매니폴드 학습의 하나로 고차원 데이터의 시각화가 목적입니다. t-SNE(Stochastic Neighbor Embedding)에서 stochastic이 들어간 것을 보면 확률적으로 나타낸다는 것을 알 수 있습니다.
t-SNE는 고차원 공간에서 두 개체가 이웃을 확률 p와 저차원 공간에서 두 개체가 이웃일 확률 q의 분포 차이가 최소가 되게 학습하는 방법으로 차원 축소를 수행하게됩니다. 이때 두 확률 분포의 유사도를 측정하는 지표로 Kullback-Leibler divergence를 사용합니다. 이 지표는 두 분포가 다르면 1, 동일하면 0을 같게 되고 따라서 아래 비용 함수를 최소화하는 방향으로 학습을 진행합니다.
$Cost = \sum_{i}KL(P_{i}||Q_{i})$

          $= \sum_{i}\sum_{j}p_{ij}log\frac{p_{ij}}{q_{ij}}$

$\frac{\theta C}{\theta y_{i}}=4\sum_{j}(y_{j} - y_{i})(p_{ij}-q{ij})$
최종적으로 구하고자 하는 값은 저차원에 임베딩된 좌표값인 $y_{i}$ 입니다. 처음 고차원 개체에서 저차원으로 $y_{i}$를 랜덤으로 초기화하고 SNE는 gradient descent를 이용해서 $y_{i}$를 업데이트하게 됩니다.

- Auto-encoder
오토 인코더는 encoder와 decoder로 이루어진 모델로 입력 데이터를 encoder를 이용해 저차원의 잠재공간 (latent space)에 압축하고 이를 decoder로 원래의 입력 데이터와 동일하게 복원시키는 구조입니다. 즉, 입력 데이터와 동한 데이터를 생성할 수 있는 encoder와 decoder를 학습시킨다고 볼 수 있습니다. 
여기서 우리는 학습된 encoder를 활용해서 데이터의 차원 축소가 가능하게 됩니다.

![화면 캡처 2022-01-09 154301](https://user-images.githubusercontent.com/44192730/148672076-bedbc5f0-6ff9-49bb-baa8-f45ebfb32301.png)

