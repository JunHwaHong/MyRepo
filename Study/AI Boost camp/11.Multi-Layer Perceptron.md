# Multi-Layer Perceptron의 이해

## Perceptron
- 각 차원의 입력값에 대응하는 가중치를 곱해 출력 값을 얻는 알고리즘입니다.
- 아래 그림을 보면 기본적인 Perceptron의 구조로 다음과 같은 계산 과정을 거칩니다.

1. n차원의 벡터를 입력값으로 받는다.
2.  n차원의 입력값 x값 $x_{1} \sim x_{n}$에 각각의 가중치 $w_{1} \sim w_{n}$가 곱해진다. (벡터의 내적 계산)
3. 2의 결과 값들이 bias와 함께 더해진다.
4. activation 함수를 활용해서 특정 threshold를 기준으로 결과값 y를 출력한다.

![화면 캡처 2022-01-09 204005](https://user-images.githubusercontent.com/44192730/148680658-21c88643-724f-4d1f-a3b1-eae62b9ec703.png)

### Perceptron의 한계점
- 기본적으로 선형 모델과 비슷한 구조를 가지기 때문에 입력값 끼리의 곱셈 또는 나눗셈과 같은 복잡한 계산이 불가능
- 복잡한 모델은 구현이 불가능하다는 것 입니다.
- 이러한 문제점을 해결하기 위해서 등장한 것이 바로 Multi-Layer Perceptron 입니다. 
- Multi-Layer Perceptron은 아래 그림과 같은 구조를 갖는데 쉽게 이해하자면 여러개의 Perceptron을 쌓아서 복잡한 표현을 가능하게 만든 모델
- ![화면 캡처 2022-01-09 204234](https://user-images.githubusercontent.com/44192730/148680761-490fc8f3-6c90-497c-a425-93af952a81ae.png)

### Logic Gate Problem
- Single-Layer Perceptron은 논리 게이트 문제에서 AND, OR 문제는 간단하게 선형적으로 해결할 수 있지만 XOR 문제는 해결할 수 없다.
- 아래 그림의 XOR 문제는 선형적인 모델(Single-Layer Perceptron)로는 풀 수 없다는 것을 알 수 있다.

![화면 캡처 2022-01-09 204455](https://user-images.githubusercontent.com/44192730/148680816-537fa288-36b0-4088-8f31-ce1b57d9915d.png)

## Multi-Layer Perceptron의 구조

- Input Layer - Hidden Layer - Output Layer
(Hiddel Layer가 많아지면 우리가 흔히 말하는 Deep Learning이라고 할 수 있다.)
- Parameters: 실제 모델 학습에서 업데이트되며 학습이 이루어지는 변수.
    - Weights
    - Bias
- Activation function: 모델에 non-linearity(비선형성)을 주면서 모델이 더 복잡하고 풍부한 표현을 가능하게 해주는 역할을 한다.
- Loss function: 모델을 학습을 하기 위해서 현재의 모델을 평가하는 함수.

![화면 캡처 2022-01-09 204603](https://user-images.githubusercontent.com/44192730/148680850-8d2897ea-f837-45ae-8d3d-061c600030ef.png)

### MLP 동작 방식

- Forward Pass
    - 주어진 입력값을 모델을 통해 출력값을 **추론**하는 과정입니다.
    - 입력받은 벡터에 대해서 parameters(weights, bias, activation function)을 이용한 출력을 함수의 형태로 얻습니다.
    - 실제 입력값에 대해서 원하는 결과값(true label)을 추론하는 과정이라고 볼 수 있습니다.

- Backward Pass
    - Forward Pass로 얻은 출력값(추론 결과값)과 해당 입력값의 실제 출력값(true label)을 이용해 loss를 계산하고 전체 모델을 **학습**하는 과정입니다.
    - Loss로부터 얻은 error 값을 back propagation이라는 알고리즘으로 모든 parameter에 전달하는 역할을 하며 이렇게 전달된 error 값을 통해 모든 parameter를 업데이트하고 이 과정을 우리는 모델을 학습 시킨다고 합니다.

---
# Deep Learning의 등장
1. Deep Learning의 역사
    
    MLP는 일찍 등장했지만, 하드웨어적인 계산 자원이 부족하고, 효율적인 학습 방법도 잘 몰랐기 때문에 큰 주목을 받지 못했다. (예: layer가 많아질수록 parameter의 수가 많아져서 학습의 방법이 섬세해져야 한다 그런데 과거에는 이를 해결할 방법이 없었다.)
    
    - 20세기 중반: perception 알고리즘, stochastic gradient descent 알고리즘(perceptron을 학습, 이 sdg 알고리즘은 현재도 많이 쓰이고 있고, optimizer에 주축을 이루고 있는 알고리즘)
    - 20세기 말: neural network 구조, back propagation 알고리즘(error signal을 output layer에서 input layer 방향으로 보내서 , back propagation으로 parameter를 업데이틑 하는 알고리즘)
        
![화면 캡처 2022-01-09 204852](https://user-images.githubusercontent.com/44192730/148680931-3ee1b612-60a1-454b-91c0-56a59cb1019e.png)

    → 이렇게 hidden layer가 9개만 되어도 weight가 굉장히 많아진다.(parameter 개수 약 300개) → 그래서 과거에는 효율적으로 학습시킬 방법이 없었다.
    
2. Deep Learning의 부활
    1. hardware 기술이 발전하면서 컴퓨팅 속도가 빨라고 다양한 학습 기법들이 실제로 개발되면서 이전에는 학습시키기 어려웠던 MLP들을 학습시킬 수 있게 됨
        - 하드웨어: GPU 발전이 가장 큰 기여, TPU, cloud computing(colab)
        - 학습 기법: backpropagation, initialization(parameter의 초기값), activation, regularization(test data 성능 올리는) 등등
        - big data 시대가 열림(데이터 수집, 저장하는 방식이 굉장히 발달함)
        
    2. 점점 많은 수의 층을 쌓아 모델을 구성하여 **표현력을 키우는 방향**으로 접근
        
![화면 캡처 2022-01-09 205033](https://user-images.githubusercontent.com/44192730/148681002-ad8d0268-79d5-42aa-a004-44360ba1ae6f.png)
        
        - 이 때 당시 parameter의 갯수를 획기적으로 늘리는 모델
        - 이 모델 구조를 이용해서 imageNet이란 데이터 셋 classfication을 하는 논문 등장

3. Successful Applications
    1. DeepDream : 원래의 그림에 새로운 feature를 넣는 것
    2. Neural Style : 하나의 이미지를 다른 이미지에 덮어 씌우는 것

![화면 캡처 2022-01-09 205402](https://user-images.githubusercontent.com/44192730/148681099-d55e97fe-c195-4b68-a1c8-1a79bfc3be94.png)
