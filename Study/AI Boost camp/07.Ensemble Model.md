# Ensemble Methods
![화면 캡처 2022-01-09 153028](https://user-images.githubusercontent.com/44192730/148671807-63bba725-628f-453d-8485-5e26af6c8d4f.png)
- 여러 개의 모델(weak learners)들을 결합해서 하나의 강력한 모델(strong learner)을 만드는 방법
- Random Forest 는 Decision Tree에 특화된 방법이지만, Bagging 과 Boosting 은 다른 모델들(딥러닝 기법 포함)에도 적용할 수 있다.

---
## Bagging
![화면 캡처 2022-01-09 153120](https://user-images.githubusercontent.com/44192730/148671829-2dc11fb0-d7a9-42e0-9dff-e3fb5edccf60.png)
![화면 캡처 2022-01-09 153129](https://user-images.githubusercontent.com/44192730/148671832-5f3f40d3-ee8a-417f-8b36-92c8e601783f.png)
- Weak learners의 예측의 합의 평균으로 최종 예측을 하는 것이 Bagging의 핵심 아이디어
    - 여러가지 decision tree의 평균 예측값은 variance가 작다. ⇒ overfitting 완화
    - (각 weak learner가 독립적이여야 variance가 작아짐)
    - 각 weak learner는 원래의 데이터셋에서 random으로 복원추출한 데이터셋으로 학습
- $\hat{f}_{bag}(x) = \frac1B \sum_{b=1}^{B}\hat{f}^{*b}(x)$ : (regression)
    - $\hat f^{*b}(x)$ 가 하나의 모델(weak learner): b번째 bootstrapped 된 데이터셋에 학습된 모델
    - Bootstrapped 된 데이터셋은 총 B개
- 최종 예측은 regression 같은 경우 평균을 취하고, classification 의 경우 vote로 결정
- Bootstrapping 복원추출
    - 복원추출을 하기 때문에 전체 데이터셋의 1/3 정도는 한 번도 뽑히지 않게 된다. (by 수학적 계산)
    - 따라서 이 데이터를 Out-of-Bag, 줄여서 OOB라 부르고
    - OOB를 validation set 으로 사용해 validation error 를 구할 수 있다
- Bagging 을 쓰게 되었을 때는 여러 개의 모델을 쓰고, 각 모델의 모양이 다르기 때문에 설명력이 낮아진다.
    - 하나의 모델을, ex) a single decision tree를 쓰는 경우 모델를 설명하기 쉽다. (설명력이 높음)
    - Bagging을 하게 되면 설명력 감소, 예측 성능은 증가하게 되는데
    - Sum of squared error(regression) 혹은 Gini index, Entropy와 같은 node impurity(classification)를 이용해서 예측 변수들의 중요도를 확인한다. (at 모든 tree에서의 각 예측 변수의 split)
    - 그 결과로 Variable Importance Measure를 계산해서, 어떤 variable이 예측에 중요한지 나타낼 수 있다.
    
---
## Random Forest
![화면 캡처 2022-01-09 153216](https://user-images.githubusercontent.com/44192730/148671839-09d929e4-e847-4746-aad4-429d8b590582.png)
- Random Forest는 Bagging의 일종
- Bagging의 핵심 아이디어는 각 weak learner의 예측을 합쳐서 variance 를 줄이는 것
    - variance을 줄이려면 각 weak learner는 독립적이여야 한다.
    - 하지만 bootstrapping 된 데이터들은 독립적이지 못하고 서로 correlation이 높다.
    - 따라서 각 weak learners를 decorrelate 해주기 위해 전체 예측 변수 p개 중 랜덤하게 일부 m개의 예측변수만 사용하여 모델을 만든다.
    - 보통 $m \approx \sqrt{p}$ 로 사용한다.

---
## Boosting
![화면 캡처 2022-01-09 153310](https://user-images.githubusercontent.com/44192730/148671851-d4e3d135-02b7-4cf8-9e92-f73e8e6e4a27.png)
![화면 캡처 2022-01-09 153341](https://user-images.githubusercontent.com/44192730/148671859-b3fbba5f-24f6-4307-9452-7941bf59e69c.png)
- Boosting은 각 weak learner가 독립적으로 학습되는 것이 아닌, 그 전까지 학습된 정보들을 이용하여 순차적으로 학습이 진행된다.
- 각 학습이 진행되는데, 그 학습은 잔차에 맞춰 피팅이 된다.
    - 잔차(residual)은 처음은 target variable과 같고 학습이 진행되며 업데이트된다.
    - 각 학습이 그 다음 모델에 반영되는 비율, 모델의 예측값이 그 전 잔차에 반영되는 비율은 Shrinkage parameter $\gamma$ 로 조절된다.
- 학습을 시키는 사람이 조절해줘야하는 3개의 hyperparameter가 있다.
    - B = 얼마나 많이 업데이트가 진행될 것인지 = weak learner의 수
        - boosting은 '잔차'에 피팅을 시키기 때문에
        - B가 클 수록 모든 잔차가 0으로 되고자 피팅이 된다 ⇒ Overfitting이 될 수 있다.
    - $\gamma$ = Shrinkage Parameter, 각 weak learner의 결과 반영 비율 = 학습 속도(0.1 ~ 0.01)
        - 학습 속도가 작아지면 B가 커야한다.
    - d = split 횟수, boosted tree의 복잡도를 조절한다.
        - d가 커지면 복잡도가 높아져서 bias는 줄어들지만 variance가 높아진다.
        - boosting을 통해 bias를 줄이기 때문에 클 필요는 없다.

---
## Bagging vs Boosting
![화면 캡처 2022-01-09 153436](https://user-images.githubusercontent.com/44192730/148671878-dc209457-5ef6-43db-a315-dfea58d1a234.png)
- Bagging
    - 각 모델이 bootstrapped dataset에 학습되기 때문에 '병렬' 앙상블이라고 하고, 각 모델이 독립적이라고 함
    - 각 독립된 모델의 예측의 합을 이용해서 최종 예측을 하기 때문에, variance를 낮추는데 특화
        - 따라서 High variance, low bias 모델을 사용기에 좋다
        - pruning 를 하지 않는 tree를 합쳐서 만듦
- Boosting
    - 이 전 모델의 잔차를 고려해서 순차적으로 학습됨
    - 잔차에 피팅이 되기에 bias 를 낮추는데 특화
        - 따라서 low variance, high bias 모델에 사용하기 좋다
    - Boosting 은 각 데이터마다 잔차에 따라 가중치(weight)를 줄 수 있고 샘플링을 할 수도 있다.
    - Bagging 과 다르게 학습된 모델을 keep하는 것이 아니라 evaluate을 해서, 모델을 keep하고 최종 모델과 잔차값에 업데이트한다

---
## Summary
![화면 캡처 2022-01-09 153512](https://user-images.githubusercontent.com/44192730/148671893-571b9b0b-77a0-4568-aa56-d6dfeecb1d04.png)
